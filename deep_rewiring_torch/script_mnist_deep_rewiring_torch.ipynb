{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-01-04T17:27:46.651311Z",
     "end_time": "2024-01-04T17:27:48.609694Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import crystalball_gen\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_data_transform = T.ToTensor()\n",
    "train_data_target_transform = T.Compose([\n",
    "    lambda x: torch.LongTensor([x]),\n",
    "    lambda x: F.one_hot(x, -1)\n",
    "])\n",
    "train_dataset = MNIST(\n",
    "    root='data/MNIST',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=train_data_transform,\n",
    "    target_transform=train_data_target_transform,\n",
    ")\n",
    "train_kwargs = {\n",
    "    'batch_size': 256,\n",
    "}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
    "\n",
    "# Test dataset\n",
    "test_data_transform = None\n",
    "test_data_target_transform = None\n",
    "test_dataset = MNIST(\n",
    "    root='data/MNIST',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=test_data_transform,\n",
    "    target_transform=test_data_target_transform,\n",
    ")\n",
    "test_kwargs = {\n",
    "    'batch_size': 256,\n",
    "}\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T16:46:06.794670Z",
     "end_time": "2024-01-04T16:46:08.425003Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for data, label in train_loader:\n",
    "    print(f\"Data: {data.shape}\\nLabel: {label}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "n_epochs = 10\n",
    "n_minibatch = 10\n",
    "print_every = 100\n",
    "n_1 = 300\n",
    "n_2 = 100\n",
    "\n",
    "p01 = .01\n",
    "p02 = .03\n",
    "p0out = .3\n",
    "l1 = 1e-5\n",
    "gdnoise = 1e-5\n",
    "lr = 0.5\n",
    "lr_epoch_decay = 0.8\n",
    "\n",
    "# Useful constants\n",
    "dtype = torch.float32\n",
    "n_pixels = 28 * 28\n",
    "n_out = 10\n",
    "n_image_per_epoch = x_train.shape[0]\n",
    "n_iter = n_epochs * n_image_per_epoch // n_minibatch\n",
    "mnist_epoch_completed = 0\n",
    "\n",
    "# Define the number of neurons per layer\n",
    "layer_names = [\"layer1\", \"layer2\", \"outlayer\"]\n",
    "sparsity_list = [p01, p02, p0out]\n",
    "nb_non_zero_coeff_list = [n_pixels * n_1 * p01, n_1 * n_2 * p02, n_2 * n_out * p0out]\n",
    "nb_non_zero_coeff_list = {layer_names[idx] : int(n) for idx,n in enumerate(nb_non_zero_coeff_list)}\n",
    "\n",
    "x = torch.tensor() # TODO: autograd?\n",
    "y = torch.tensor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def weight_sampler_strict_number(n_in, n_out, nb_non_zero, dtype=torch.float32):\n",
    "    w_0 = rd.randn(n_in,n_out) / np.sqrt(n_in) # initial weight values\n",
    "\n",
    "    # Generate the random mask\n",
    "    is_con_0 = np.zeros((n_in,n_out),dtype=bool)\n",
    "    ind_in = rd.choice(np.arange(n_in),size=nb_non_zero)\n",
    "    ind_out = rd.choice(np.arange(n_out),size=nb_non_zero)\n",
    "    is_con_0[ind_in,ind_out] = True\n",
    "\n",
    "    # Generate random signs\n",
    "    sign_0 = np.sign(rd.randn(n_in,n_out))\n",
    "\n",
    "    # Define the matrices\n",
    "    theta = torch.tensor(np.abs(w_0) * is_con_0, dtype=dtype, requires_grad=True)\n",
    "    w_sign = torch.tensor(sign_0, dtype=dtype, requires_grad=True)\n",
    "    is_connected = torch.greater(theta, 0)\n",
    "    w = torch.where(is_connected, input=w_sign * theta, out=torch.zeros((n_in, n_out), dtype=dtype))\n",
    "\n",
    "    return w, w_sign, theta, is_connected"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T17:27:57.792956Z",
     "end_time": "2024-01-04T17:27:57.809774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assert_connection_number(theta, targeted_number):\n",
    "    '''\n",
    "    Function to check during the tensorflow simulation if the number of connection in well defined after each simulation.\n",
    "    :param theta:\n",
    "    :param targeted_number:\n",
    "    :return:\n",
    "    '''\n",
    "    th = theta.item()\n",
    "    is_con = torch.greater(th, 0)\n",
    "\n",
    "    nb_is_con = is_con.type(torch.int32).sum()\n",
    "    assert_is_con = torch.equal(nb_is_con, targeted_number)\n",
    "\n",
    "    return assert_is_con"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rewiring(theta: torch.Tensor, target_nb_connection, epsilon=1e-12):\n",
    "    th = theta.item()\n",
    "    is_con = torch.greater(th, 0)\n",
    "\n",
    "    n_connected = is_con.type(torch.int32).sum()\n",
    "    nb_reconnect = target_nb_connection - n_connected\n",
    "    nb_reconnect = torch.max(nb_reconnect, 0)\n",
    "\n",
    "    reconnect_candidate_coord = torch.where(\n",
    "        torch.logical_not(is_con),\n",
    "        input=torch.full(is_con.shape, fill_value=1.0),\n",
    "        other=torch.full(is_con.shape, fill_value=0.0)\n",
    "    ).nonzero() # Apply the conditions on the matrix to get a binary mask, then the indices of non-zero values\n",
    "\n",
    "    n_candidates = reconnect_candidate_coord.shape[0]\n",
    "    reconnect_sample_id = torch.randperm(n_candidates)[:nb_reconnect]\n",
    "    reconnect_sample_coord = torch.gather(reconnect_candidate_coord, 0, reconnect_sample_id)\n",
    "\n",
    "    # Apply rewiring\n",
    "    reconnect_vals = torch.full([nb_reconnect], epsilon)\n",
    "    reconnect_op = theta.scatter_(0, reconnect_sample_coord, reconnect_vals)\n",
    "\n",
    "    connection_check = assert_connection_number(theta, target_nb_connection) # TODO\n",
    "\n",
    "    return reconnect_op"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this context theta is an absolute weight value which is positive if the node is connected. 'w' is the signed theta\n",
    "theta = np.abs(w_0) * is_con_0\n",
    "w = w_sign * theta if is_connected else 0\n",
    "\n",
    "TODO: extract weights, thetas, grad from layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m     logits_predict \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(z_2, w_out)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits_predict\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDeepR\u001B[39;00m(\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, layer_1_dim\u001B[38;5;241m=\u001B[39m(n_pixels, n_1), layer_2_dim\u001B[38;5;241m=\u001B[39m(n_1, n_2), layer_out_dim\u001B[38;5;241m=\u001B[39m(n_2, n_out), nb_non_zero_coeff_list\u001B[38;5;241m=\u001B[39mnb_non_zero_coeff_list):\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Define layers\n",
    "def layer1():\n",
    "    W_1, _, th1, _ = weight_sampler_strict_number(n_pixels, n_1, nb_non_zero_coeff_list[0])\n",
    "    a_1 = torch.matmul(x, W_1)\n",
    "    z_1 = torch.nn.ReLU(a_1)\n",
    "    return z_1\n",
    "\n",
    "def layer2(z_1):\n",
    "    W_2, _, th2, _ = weight_sampler_strict_number(n_1, n_2, nb_non_zero_coeff_list[1])\n",
    "    a_2 = torch.matmul(z_1, W_2)\n",
    "    z_2 = torch.nn.ReLU(a_2)\n",
    "    return z_2\n",
    "\n",
    "def out_layer(z_2):\n",
    "    w_out, _, th_out, _ = weight_sampler_strict_number(n_2, n_out, nb_non_zero_coeff_list[2])\n",
    "    logits_predict = torch.matmul(z_2, w_out)\n",
    "    return logits_predict\n",
    "\n",
    "class DeepR(torch.nn.Module):\n",
    "    def __init__(self, layer_1_dim=(n_pixels, n_1), layer_2_dim=(n_1, n_2), layer_out_dim=(n_2, n_out), nb_non_zero_coeff_list=nb_non_zero_coeff_list):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.nb_non_zero_coeff_list = nb_non_zero_coeff_list\n",
    "\n",
    "        # Custom weight initialization\n",
    "        layer1 = torch.nn.Linear(in_features=layer_1_dim[0], out_features=layer_1_dim[1])\n",
    "        layer1.weight = torch.nn.Parameter(rd.randn(layer_1_dim[0],layer_1_dim[1]) / np.sqrt(layer_1_dim[0]))\n",
    "\n",
    "        layer2 = torch.nn.Linear(in_features=layer_2_dim[0], out_features=layer_2_dim[1])\n",
    "        layer2.weight = torch.nn.Parameter(rd.randn(layer_2_dim[0],layer_2_dim[1]) / np.sqrt(layer_2_dim[0]))\n",
    "\n",
    "        outlayer = torch.nn.Linear(in_features=layer_out_dim[0], out_features=layer_out_dim[1])\n",
    "        outlayer.weight = torch.nn.Parameter(rd.randn(layer_out_dim[0],layer_out_dim[1]) / np.sqrt(layer_out_dim[0]))\n",
    "\n",
    "        self.layers = torch.nn.Sequential(OrderedDict[\n",
    "            (layer_names[0], layer1),\n",
    "            (\"relu1\", torch.nn.ReLU()),\n",
    "            (layer_names[1], layer2),\n",
    "            (\"relu2\", torch.nn.ReLU()),\n",
    "            (layer_names[2], outlayer)\n",
    "        ])\n",
    "\n",
    "        # Meta inf\n",
    "        self._linear_layers = [layer1, layer2, outlayer]\n",
    "        self.theta_list = []\n",
    "        self.weight_list = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "    def meta_update(self):\n",
    "        is_first_run = not self.theta_list and not self.weight_list\n",
    "        for idx, layer in enumerate(self._linear_layers):\n",
    "            w, w_sign, th, is_conn = self.get_underlying_matrices(layer=layer)\n",
    "\n",
    "            if is_first_run:\n",
    "                self.theta_list.append(th)\n",
    "                self.weight_list.append(w)\n",
    "            else:\n",
    "                # Custom SGD impl with noise and L1 still in progress, should be relocated to a different function/optimizer (sparse_sgd)\n",
    "                # Gradient computation should be done for only active connections, default autograd is overkill probably ref. deep rewiring paper\n",
    "                [torch.add(th, lr * mask_connected(th) * noise_update(th)) for th in theta_list]# add_gradient_op_list\n",
    "\n",
    "    def get_underlying_matrices(self, layer_name, layer=None):\n",
    "        layer = layer if layer is not None else self.layers[layer_name]\n",
    "        w_0 = layer.weight\n",
    "\n",
    "        # Generate the random mask\n",
    "        ind_in = rd.choice(np.arange(layer.in_features),size=self.nb_non_zero_coeff_list[layer_name])\n",
    "        ind_out = rd.choice(np.arange(layer.out_features),size=self.nb_non_zero_coeff_list[layer_name])\n",
    "\n",
    "        is_con_0 = np.zeros((layer.in_features, layer.out_features), dtype=bool)\n",
    "        is_con_0[ind_in,ind_out] = True\n",
    "\n",
    "        # Generate random signs\n",
    "        sign_0 = np.sign(rd.randn(layer.in_features,layer.out_features))\n",
    "\n",
    "        # Define the matrices\n",
    "        theta = torch.tensor(np.abs(w_0) * is_con_0, dtype=dtype, requires_grad=True)\n",
    "        w_sign = torch.tensor(sign_0, dtype=dtype, requires_grad=True)\n",
    "        is_connected = torch.greater(theta, 0)\n",
    "        w = torch.where(\n",
    "                is_connected,\n",
    "                input=w_sign * theta,\n",
    "                out=torch.zeros((layer.in_features, layer.out_features),dtype=dtype)\n",
    "        )\n",
    "\n",
    "        return w, w_sign, theta, is_connected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calc loss (softmax_cross_entropy_with_logits in tf)\n",
    "def loss(logits_pred, y):\n",
    "    return torch.nn.functional.cross_entropy(logits_pred, y).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_connected = lambda th: torch.greater(th, 0).type(torch.float32)\n",
    "noise_update = lambda th: torch.normal(std=gdnoise, size=th.size())\n",
    "\n",
    "theta_list = []\n",
    "add_gradient_op_list ="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "def train():\n",
    "    optimizer = torch.optim.SGD(lr=lr) # TODO: model params\n",
    "    optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stats and results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
